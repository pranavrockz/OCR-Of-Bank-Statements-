{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU93fNk_rMQ9",
        "outputId": "c7d12556-704c-451f-f47e-6193346dab34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (11.0.0)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.17.0\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.41.1-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: firebase-admin in /usr/local/lib/python3.10/dist-packages (6.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.25.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: cachecontrol>=0.12.14 in /usr/local/lib/python3.10/dist-packages (from firebase-admin) (0.14.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.7.8 in /usr/local/lib/python3.10/dist-packages (from firebase-admin) (2.155.0)\n",
            "Requirement already satisfied: google-cloud-storage>=1.37.1 in /usr/local/lib/python3.10/dist-packages (from firebase-admin) (2.19.0)\n",
            "Requirement already satisfied: pyjwt>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.5.0->firebase-admin) (2.10.1)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=1.22.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (2.19.2)\n",
            "Requirement already satisfied: google-cloud-firestore>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from firebase-admin) (2.19.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (1.18.4)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from cachecontrol>=0.12.14->firebase-admin) (1.1.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (1.66.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (1.25.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (2.27.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (1.68.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (1.62.3)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.7.8->firebase-admin) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.7.8->firebase-admin) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.7.8->firebase-admin) (4.1.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-firestore>=2.19.0->firebase-admin) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage>=1.37.1->firebase-admin) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage>=1.37.1->firebase-admin) (1.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.5.0->firebase-admin) (43.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.5.0->firebase-admin) (1.17.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client>=1.7.8->firebase-admin) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]>=2.5.0->firebase-admin) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (0.6.1)\n",
            "Downloading streamlit-1.41.1-py2.py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.41.1 watchdog-6.0.0\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\n",
            "added 22 packages in 3s\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0KCollecting pyngrok\n",
            "  Downloading pyngrok-7.2.2-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.2\n",
            "Collecting paddlepaddle\n",
            "  Downloading paddlepaddle-2.6.2-cp310-cp310-manylinux1_x86_64.whl.metadata (8.6 kB)\n",
            "Collecting paddleocr\n",
            "  Downloading paddleocr-2.9.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (11.0.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (4.4.2)\n",
            "Collecting astor (from paddlepaddle)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting opt-einsum==3.3.0 (from paddlepaddle)\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from paddlepaddle) (4.25.5)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (from paddleocr) (2.0.6)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from paddleocr) (0.25.0)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.10/dist-packages (from paddleocr) (0.4.0)\n",
            "Collecting pyclipper (from paddleocr)\n",
            "  Downloading pyclipper-1.3.0.post6-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting lmdb (from paddleocr)\n",
            "  Downloading lmdb-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from paddleocr) (4.67.1)\n",
            "Collecting rapidfuzz (from paddleocr)\n",
            "  Downloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from paddleocr) (4.10.0.84)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from paddleocr) (4.10.0.84)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from paddleocr) (3.0.11)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from paddleocr) (6.0.2)\n",
            "Collecting python-docx (from paddleocr)\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from paddleocr) (4.12.3)\n",
            "Requirement already satisfied: fonttools>=4.24.0 in /usr/local/lib/python3.10/dist-packages (from paddleocr) (4.55.3)\n",
            "Collecting fire>=0.3.0 (from paddleocr)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from paddleocr) (2.32.3)\n",
            "Collecting albumentations==1.4.10 (from paddleocr)\n",
            "  Downloading albumentations-1.4.10-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting albucore==0.0.13 (from paddleocr)\n",
            "  Downloading albucore-0.0.13-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.13->paddleocr) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.13->paddleocr) (4.12.2)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.13->paddleocr) (4.10.0.84)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.4.10->paddleocr) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.4.10->paddleocr) (1.6.0)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations==1.4.10->paddleocr) (2.10.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire>=0.3.0->paddleocr) (2.5.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->paddleocr) (3.4.2)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->paddleocr) (2.36.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->paddleocr) (2024.12.12)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->paddleocr) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->paddleocr) (0.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->paddleocr) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->paddlepaddle) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->paddlepaddle) (0.14.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug->paddleocr) (1.17.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from imgaug->paddleocr) (3.8.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx->paddleocr) (5.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->paddleocr) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->paddleocr) (2.2.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations==1.4.10->paddleocr) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations==1.4.10->paddleocr) (2.27.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.2->albumentations==1.4.10->paddleocr) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.2->albumentations==1.4.10->paddleocr) (3.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->paddlepaddle) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->paddlepaddle) (1.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->paddleocr) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->paddleocr) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->paddleocr) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->paddleocr) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->paddleocr) (2.8.2)\n",
            "Downloading paddlepaddle-2.6.2-cp310-cp310-manylinux1_x86_64.whl (126.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading paddleocr-2.9.1-py3-none-any.whl (544 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m544.7/544.7 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading albucore-0.0.13-py3-none-any.whl (8.5 kB)\n",
            "Downloading albumentations-1.4.10-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.9/161.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading lmdb-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (912 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.2/912.2 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=9cdec2529b01763bc3353b249caf218ea656af6f3ffeee9f88ec66527a21f76d\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\n",
            "Successfully built fire\n",
            "Installing collected packages: pyclipper, lmdb, rapidfuzz, python-docx, opt-einsum, fire, astor, albucore, paddlepaddle, albumentations, paddleocr\n",
            "  Attempting uninstall: opt-einsum\n",
            "    Found existing installation: opt_einsum 3.4.0\n",
            "    Uninstalling opt_einsum-3.4.0:\n",
            "      Successfully uninstalled opt_einsum-3.4.0\n",
            "  Attempting uninstall: albucore\n",
            "    Found existing installation: albucore 0.0.19\n",
            "    Uninstalling albucore-0.0.19:\n",
            "      Successfully uninstalled albucore-0.0.19\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 1.4.20\n",
            "    Uninstalling albumentations-1.4.20:\n",
            "      Successfully uninstalled albumentations-1.4.20\n",
            "Successfully installed albucore-0.0.13 albumentations-1.4.10 astor-0.8.1 fire-0.7.0 lmdb-1.5.1 opt-einsum-3.3.0 paddleocr-2.9.1 paddlepaddle-2.6.2 pyclipper-1.3.0.post6 python-docx-1.1.2 rapidfuzz-3.11.0\n",
            "Collecting cohere\n",
            "  Downloading cohere-5.13.5-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.28.1)\n",
            "Collecting httpx-sse==0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.10.3)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.27.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.21.0)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.2.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<1,>=0.15->cohere) (0.27.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.67.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.2)\n",
            "Downloading cohere-5.13.5-py3-none-any.whl (250 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.7/250.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading fastavro-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: types-requests, parameterized, httpx-sse, fastavro, cohere\n",
            "Successfully installed cohere-5.13.5 fastavro-1.10.0 httpx-sse-0.4.0 parameterized-0.9.0 types-requests-2.32.0.20241016\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.9.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.5.2 (from gradio)\n",
            "  Downloading gradio_client-1.5.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.3)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.8.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.5.2->gradio) (14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.27.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.9.1-py3-none-any.whl (57.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.5.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.4/320.4 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.8.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.6 ffmpy-0.5.0 gradio-5.9.1 gradio-client-1.5.2 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.8.6 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.41.3 tomlkit-0.13.2 uvicorn-0.34.0\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pdf2image\n",
        "!pip install streamlit firebase-admin requests\n",
        "!npm install -g localtunnel\n",
        "!pip install pyngrok\n",
        "!pip install paddlepaddle paddleocr\n",
        "!pip install cohere\n",
        "!pip install numpy\n",
        "!pip install transformers\n",
        "!pip install pillow\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install gradio\n",
        "!pip install openpyxl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJiVpzwBq5LC",
        "outputId": "79acfc68-ce41-4b6c-c3da-1f9e163dc0fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from paddleocr import PaddleOCR, draw_ocr\n",
        "from PIL import Image, ImageEnhance, ImageFilter, ImageDraw, UnidentifiedImageError\n",
        "from pdf2image import convert_from_bytes\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from firebase_admin import credentials, firestore\n",
        "import firebase_admin\n",
        "import cohere\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import io\n",
        "import os\n",
        "import traceback\n",
        "\n",
        "@st.cache_resource  # Cache the PaddleOCR model\n",
        "def load_ocr_model():\n",
        "    return PaddleOCR(use_angle_cls=True, lang='en')\n",
        "\n",
        "@st.cache_data  # Cache the Cohere API call results\n",
        "def cached_search_with_cohere(extracted_text, prompting_type, custom_keywords, document_type):\n",
        "    return search_with_cohere(extracted_text, prompting_type, custom_keywords, document_type)\n",
        "\n",
        "# Initialize Firebase\n",
        "cred = credentials.Certificate(\"serviceAccountKey.json\")  # Replace with your Firebase service account key\n",
        "#firebase_admin.initialize_app(cred)\n",
        "\n",
        "# Initialize Cohere API with your API key\n",
        "co = cohere.Client(\"aeYHJcrlPDDgIuO6w0EeOFtyQ0gkdV2jj3cHQT1G\")  # Replace with your actual Cohere API key\n",
        "db = firestore.client()\n",
        "\n",
        "# Function to preprocess the image\n",
        "def preprocess_image(image, contrast, sharpen, median_filter):\n",
        "    image = image.convert(\"RGB\")\n",
        "    image = ImageEnhance.Contrast(image).enhance(contrast)  # Enhance contrast\n",
        "    for _ in range(sharpen):\n",
        "        image = image.filter(ImageFilter.SHARPEN)  # Sharpen the image\n",
        "    if median_filter > 1:\n",
        "        image = image.filter(ImageFilter.MedianFilter(size=median_filter))  # Denoise\n",
        "    return image\n",
        "\n",
        "# Function to query Firestore and get image URLs from a specified collection\n",
        "def get_image_urls_from_firestore(collection_name, num_images):\n",
        "    try:\n",
        "        collection_ref = db.collection(collection_name)\n",
        "        docs = collection_ref.stream()\n",
        "        count = 0\n",
        "\n",
        "        image_urls = []\n",
        "        for doc in docs:\n",
        "            data = doc.to_dict()\n",
        "            url = data.get(\"url\")  # Assuming the URL is stored under the 'url' field\n",
        "            if url:\n",
        "                if count < num_images:\n",
        "                    image_urls.append(url)\n",
        "                    count += 1\n",
        "        return image_urls\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error retrieving data from Firestore for {collection_name}: {e}\")\n",
        "        return []\n",
        "\n",
        "# ============================\n",
        "# Configuration Section\n",
        "# ============================\n",
        "\n",
        "# Update this path based on your system\n",
        "# For Windows:\n",
        "# Global variable to store the uploaded file\n",
        "uploaded_file = None\n",
        "image = None\n",
        "\n",
        "# Predefined list of 5 keywords to always search for\n",
        "predefined_keywords = [\"Amount\", \"Date\", \"Transaction\", \"Invoice\", \"Balance\"]\n",
        "\n",
        "# Initialize OCR models\n",
        "paddle_ocr = PaddleOCR(use_angle_cls=True, lang='en')\n",
        "\n",
        "def process_single_image(image, file_name, prompting_type, custom_keywords=None, document_type=\"Invoice\"): #Process one image at a time\n",
        "    try:\n",
        "        params = {\n",
        "            \"use_angle_cls\": True\n",
        "        }\n",
        "        result = process_image_with_cohere(image, prompting_type, custom_keywords, document_type, params)\n",
        "        if isinstance(result, dict) and \"error\" in result:\n",
        "            st.error(result[\"error\"])\n",
        "            return None, None\n",
        "        response_values = parse_response_to_dict(result.split(\"\\n\\nSearch Result:\\n\")[-1])\n",
        "        plot = plot_values(response_values, file_name)\n",
        "        plot1 = plot_pie_chart(response_values, file_name)\n",
        "        return result, [plot, plot1]\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error processing image: {e}\")\n",
        "        st.error(traceback.format_exc())\n",
        "        return None, None\n",
        "\n",
        "def plot_pie_chart(values_by_keyword, image_name):\n",
        "    total_values = {}\n",
        "    for k, v_list in values_by_keyword.items():\n",
        "        total_values[k] = len(v_list)\n",
        "\n",
        "    if not total_values:\n",
        "        # Debugging: Print message if no values are found\n",
        "        print(f\"No values found for pie chart for image: {image_name}\")\n",
        "        return Image.new('RGB', (300, 200), (255, 255, 255))\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    ax.pie(total_values.values(), labels=total_values.keys(), autopct='%1.1f%%', startangle=90)\n",
        "    ax.set_title(f'Pie Chart of Keyword Values from {image_name}')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf, format='png')\n",
        "    buf.seek(0)\n",
        "    img = Image.open(buf)\n",
        "    return img\n",
        "\n",
        "# Modify the 'process_multiple_images' function to generate and save results as an Excel file\n",
        "def save_results_to_excel(results_by_keyword, output_dir=\"output\"):\n",
        "    # Create the output directory if it doesn't exist\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Flatten the results_by_keyword dictionary to create rows for each keyword and its values\n",
        "    flattened_data = []\n",
        "    for keyword, values in results_by_keyword.items():\n",
        "        for value in values:\n",
        "            flattened_data.append({\"Keyword\": keyword, \"Value\": value})\n",
        "\n",
        "    # Create a pandas DataFrame\n",
        "    df = pd.DataFrame(flattened_data)\n",
        "\n",
        "    # Generate the output Excel file path\n",
        "    output_file = os.path.join(output_dir, \"extracted_results.xlsx\")\n",
        "\n",
        "    # Write the DataFrame to an Excel file\n",
        "    df.to_excel(output_file, index=False, engine='openpyxl')\n",
        "\n",
        "    return output_file\n",
        "\n",
        "def plot_values(values_by_keyword, image_name):\n",
        "    avg_values = {}\n",
        "    for k, v_list in values_by_keyword.items():\n",
        "        numeric_values = []\n",
        "        for v in v_list:\n",
        "            if isinstance(v, (int, float)):\n",
        "                numeric_values.append(float(v))\n",
        "            elif isinstance(v, str):\n",
        "                # Remove non-numeric characters except for '-' and '.'\n",
        "                cleaned_value = re.sub(r'[^\\d.-]', '', v)\n",
        "                if re.match(r'^-?\\d+(\\.\\d+)?$', cleaned_value):  # Check if the string represents a float\n",
        "                    numeric_values.append(float(cleaned_value))\n",
        "\n",
        "        if numeric_values:\n",
        "            avg_values[k] = np.mean(numeric_values)\n",
        "\n",
        "    if not avg_values:\n",
        "        return Image.new('RGB', (300, 200), (255, 255, 255))  # Return a blank image if no valid values are found\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.bar(avg_values.keys(), avg_values.values())\n",
        "    ax.set_xlabel('Keyword')\n",
        "    ax.set_ylabel('Average Value')\n",
        "    ax.set_title(f'Bar Graph of Average Numeric Values from {image_name}')\n",
        "    ax.set_xticklabels(avg_values.keys(), rotation=45)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf, format='png')\n",
        "    buf.seek(0)\n",
        "    img = Image.open(buf)\n",
        "    return img\n",
        "\n",
        "def detect_with_ppocr(image, params):\n",
        "    ocr = PaddleOCR(use_angle_cls=params[\"use_angle_cls\"], lang='en')  # Use the English model for OCR\n",
        "    np_image = np.array(image)\n",
        "    result = ocr.ocr(np_image, cls=params[\"use_angle_cls\"])\n",
        "\n",
        "    # Draw bounding boxes\n",
        "    image_with_boxes = image.copy()\n",
        "    draw = ImageDraw.Draw(image_with_boxes)\n",
        "    boxes = []\n",
        "    for line in result[0]:\n",
        "        points = line[0]\n",
        "        top_left = tuple(map(int, points[0]))\n",
        "        bottom_right = tuple(map(int, points[2]))\n",
        "        text = line[1][0]\n",
        "        confidence = line[1][1] * 100  # Scale confidence to 100\n",
        "        boxes.append((top_left, bottom_right, text, confidence))\n",
        "        draw.rectangle([top_left, bottom_right], outline=\"blue\", width=2)\n",
        "\n",
        "    return image_with_boxes, boxes\n",
        "\n",
        "def extract_dynamic_keywords(document_type):\n",
        "    if document_type == \"Payslips\":\n",
        "        return [\"Salary\", \"Date\", \"Pay\", \"Tax\", \"Name\", \"Pension\", \"Insurance\"]\n",
        "    elif document_type == \"Bank Statement\":\n",
        "        return [\"Balance\", \"Transaction\", \"Withdrawals\", \"Charge\", \"Payments\"]\n",
        "    elif document_type == \"Balance Sheet\":\n",
        "        return [\"Date\", \"Assets\", \"Cash\", \"Capital\", \"Liabilities\", \"Equity\"]\n",
        "    else:\n",
        "        return predefined_keywords\n",
        "\n",
        "def process_multiple_images(files, prompting_type, custom_keywords=None, document_type=\"Invoice\"):\n",
        "    results = []\n",
        "    all_plots = []\n",
        "    all_values_by_keyword = defaultdict(list)\n",
        "    progress_bar = st.progress(0)\n",
        "    params = {\n",
        "        \"use_angle_cls\": True\n",
        "    }\n",
        "\n",
        "    for idx, file in enumerate(files):\n",
        "        try:\n",
        "            progress_bar.progress((idx + 1) / len(files))\n",
        "            if isinstance(file, dict):\n",
        "                img_byte_arr = file.get(\"data\")\n",
        "                if img_byte_arr is None:\n",
        "                    raise ValueError(\"No image data found in the file\")\n",
        "                if isinstance(img_byte_arr, io.BytesIO):\n",
        "                    image = Image.open(img_byte_arr)\n",
        "                else:\n",
        "                    image = Image.open(io.BytesIO(img_byte_arr))\n",
        "            else:\n",
        "                image = Image.open(file)\n",
        "\n",
        "            result = process_image_with_cohere(image, prompting_type, custom_keywords, document_type, params)\n",
        "            if isinstance(result, dict) and \"error\" in result:\n",
        "                st.error(result[\"error\"])\n",
        "                continue\n",
        "\n",
        "            if isinstance(result, str):\n",
        "                response_values = parse_response_to_dict(result.split(\"\\n\\nSearch Result:\\n\")[-1])\n",
        "                # Debugging: Print response values for each image\n",
        "                print(f\"Response values for image {file['name'] if isinstance(file, dict) else file.name}: {response_values}\")\n",
        "            else:\n",
        "                st.error(\"Unexpected result format\")\n",
        "                continue\n",
        "\n",
        "            st.subheader(f\"Plots for {file['name'] if isinstance(file, dict) else file.name}\")\n",
        "            bar_plot = plot_values(response_values, file['name'] if isinstance(file, dict) else file.name)\n",
        "            pie_plot = plot_pie_chart(response_values, file['name'] if isinstance(file, dict) else file.name)\n",
        "            all_plots.append((bar_plot, pie_plot, file['name'] if isinstance(file, dict) else file.name))\n",
        "\n",
        "            if isinstance(response_values, dict):\n",
        "                for keyword, values in response_values.items():\n",
        "                    all_values_by_keyword[keyword].extend(values)\n",
        "            else:\n",
        "                st.error(f\"Expected response_values to be a dictionary, got {type(response_values)}\")\n",
        "\n",
        "            results.append(f\"--- Result for Image {idx + 1} ({file['name'] if isinstance(file, dict) else file.name}) ---\\n{result}\")\n",
        "        except Exception as e:\n",
        "            results.append(f\"--- Error processing Image {idx + 1} ({file['name'] if isinstance(file, dict) else file.name}) ---\\n{e}\")\n",
        "            st.error(f\"Error: {e}\")\n",
        "            traceback.print_exc()\n",
        "    excel_file = save_results_to_excel(all_values_by_keyword)\n",
        "    return \"\\n\\n\".join(results), all_plots, excel_file\n",
        "\n",
        "def parse_response_to_dict(response_text):\n",
        "    values_by_keyword = defaultdict(list)\n",
        "    lines = response_text.split('\\n')\n",
        "    for line in lines:\n",
        "        match = re.match(r\"- (.+): Found \\d+ time\\(s\\)\\. Values: (.+)\", line.strip())\n",
        "        if match:\n",
        "            keyword = match.group(1).strip()\n",
        "            values_str = match.group(2).strip()\n",
        "            if values_str.lower() != \"not applicable\":\n",
        "                values = [v.strip() for v in values_str.split(',')]\n",
        "                for value in values:\n",
        "                    try:\n",
        "                        # Clean the value by removing non-numeric characters except for '-' and '.'\n",
        "                        cleaned_value = re.sub(r'[^\\d.-]', '', value)\n",
        "                        num_value = float(cleaned_value)\n",
        "                        values_by_keyword[keyword].append(num_value)\n",
        "                    except ValueError:\n",
        "                        values_by_keyword[keyword].append(value)\n",
        "            else:\n",
        "                values_by_keyword[keyword] = []\n",
        "    return dict(values_by_keyword)\n",
        "\n",
        "# Function to perform exploratory data analysis\n",
        "def exploratory_data_analysis(df):\n",
        "    st.header(\"Exploratory Data Analysis (EDA)\")\n",
        "\n",
        "    # Show basic statistics\n",
        "    st.subheader(\"Basic Statistics\")\n",
        "    st.write(df.describe())\n",
        "\n",
        "    # Plot distribution of numeric values\n",
        "    st.subheader(\"Distribution of Numeric Values\")\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    for col in numeric_cols:\n",
        "        fig, ax = plt.subplots()\n",
        "        df[col].plot(kind='hist', bins=30, ax=ax, title=f'Distribution of {col}')\n",
        "        st.pyplot(fig)\n",
        "\n",
        "    # Plot time series if there are date values\n",
        "    if 'Date' in df.columns:\n",
        "        st.subheader(\"Time Series Analysis\")\n",
        "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "        df.set_index('Date', inplace=True)\n",
        "        for col in numeric_cols:\n",
        "            fig, ax = plt.subplots()\n",
        "            df[col].plot(ax=ax, title=f'Time Series of {col}')\n",
        "            st.pyplot(fig)\n",
        "        df.reset_index(inplace=True)\n",
        "\n",
        "    # Correlation matrix\n",
        "    st.subheader(\"Correlation Matrix\")\n",
        "    corr_matrix = df.corr()\n",
        "    fig, ax = plt.subplots()\n",
        "    cax = ax.matshow(corr_matrix, cmap='coolwarm')\n",
        "    fig.colorbar(cax)\n",
        "    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns, rotation=90)\n",
        "    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n",
        "    st.pyplot(fig)\n",
        "\n",
        "    # Display correlation matrix\n",
        "    st.write(corr_matrix)\n",
        "\n",
        "# Function to search with Cohere API and ensure the response is correctly parsed\n",
        "def search_with_cohere(extracted_text, prompting_type, custom_keywords=None, document_type=\"Invoice\"):\n",
        "    try:\n",
        "        # Limit the extracted text length if it's too long\n",
        "        max_length = 2000\n",
        "        if len(extracted_text) > max_length:\n",
        "            extracted_text = extracted_text[:max_length]\n",
        "\n",
        "        # If custom_keywords are provided, join them into a string\n",
        "        keywords = \", \".join(custom_keywords) if custom_keywords else \"\"\n",
        "        prompt = f\"\"\"\n",
        "        Analyze the following text and extract the values associated with these keywords: {keywords}.\n",
        "        For each keyword, do the following:\n",
        "\n",
        "        1. Search for any substring of the keyword within the text (case-insensitive). This includes the full keyword or any part of it (e.g., for 'Salary', also find 'Total Salary', 'Salary Amount', etc.).\n",
        "        2. For each match found, provide:\n",
        "            - The number of occurrences of the keyword or its substring.\n",
        "            - The corresponding value(s) found right after or near the keyword. If a value is numeric, it might follow directly after the keyword or in the form of a unit (e.g., 'Salary: 120 USD', 'Tax 50').\n",
        "            - If a keyword is not found or does not have a corresponding value, state 'Not applicable'.\n",
        "\n",
        "        Text:\n",
        "        {extracted_text}\n",
        "\n",
        "        Return results in this format:\n",
        "        - <Keyword>: Found <Number> time(s). Values: <Extracted values or 'Not applicable'>.\n",
        "        - If no values are found or the keyword is not present, return 'Not applicable'.\n",
        "\n",
        "        If multiple matches are found for the same keyword/subkeyword, list all corresponding values.\n",
        "        \"\"\"\n",
        "\n",
        "        # Make the API call to Cohere\n",
        "        response = co.generate(\n",
        "            model='command',\n",
        "            prompt=prompt,\n",
        "            max_tokens=500\n",
        "        )\n",
        "\n",
        "        # Extract the text result from the response\n",
        "        result_text = response.generations[0].text.strip()\n",
        "        parse_response_to_dict(result_text)\n",
        "\n",
        "        # Debugging: Print the response from Cohere API\n",
        "        st.write(\"Cohere API response:\", result_text)\n",
        "\n",
        "        return result_text\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Error during keyword search with Cohere: {str(e)}\\n{traceback.format_exc()}\"}\n",
        "\n",
        "def extract_text_with_paddleocr(image, params):\n",
        "    ocr = PaddleOCR(use_angle_cls=params[\"use_angle_cls\"], lang='en')\n",
        "    image = np.array(image)\n",
        "    result = ocr.ocr(image, cls=params[\"use_angle_cls\"])\n",
        "    extracted_text = [line[1][0] for line in result[0]]\n",
        "    if not extracted_text:\n",
        "        return \"No text detected in the image.\"\n",
        "    return extracted_text\n",
        "\n",
        "def process_image_with_cohere(image, prompting_type, custom_keywords=None, document_type=\"Invoice\", params=None):\n",
        "    try:\n",
        "        extracted_data = extract_text_with_paddleocr(image, params)\n",
        "        if isinstance(extracted_data, str) and extracted_data.startswith(\"Error\"):\n",
        "            return {\"error\": f\"Error during text extraction: {extracted_data}\"}\n",
        "\n",
        "        extracted_text = \" \".join(extracted_data)\n",
        "\n",
        "        result = search_with_cohere(extracted_text, prompting_type, custom_keywords, document_type)\n",
        "        if 'error' in result:\n",
        "            return result  # Return the error dictionary directly\n",
        "\n",
        "        # Ensure the result is a string before splitting\n",
        "        if isinstance(result, str):\n",
        "            return f\"Document Type: {document_type}\\n\\nExtracted Text:\\n{extracted_text}\\n\\nSearch Result:\\n{result}\"\n",
        "        else:\n",
        "            return {\"error\": \"Unexpected result format from search_with_cohere\"}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Error: {e}\"}\n",
        "\n",
        "def convert_df_to_text(df):\n",
        "    return \"\\n\".join(df['Text'].tolist())\n",
        "\n",
        "def extract_value(prompt, result):\n",
        "    keyword = prompt.lower()  # Convert keyword to lowercase for case-insensitive comparison\n",
        "\n",
        "    # Iterate through each line in the OCR result\n",
        "    for line in result:\n",
        "        if len(line) > 1 and len(line[1]) > 0:  # Ensure the line contains text\n",
        "            text = line[1][0]  # Extract the recognized text from the OCR result\n",
        "            print(\"Detected Text Line:\", text)  # Log the detected text for debugging\n",
        "\n",
        "            # Split the text into words and store them in a list\n",
        "            words = text.split()  # Convert the text into a list of words\n",
        "\n",
        "            # Iterate over the words list to find the keyword\n",
        "            for i, word in enumerate(words):\n",
        "                if keyword in word.lower():  # Check for the keyword in the word (case-insensitive)\n",
        "                    # If the keyword is found, check the next word as the value\n",
        "                    if i + 1 < len(words):\n",
        "                        return f\"The value for '{keyword}' is: {words[i + 1]}\"\n",
        "\n",
        "    return f\"'{keyword}' not found in the image.\"\n",
        "\n",
        "# --------------------------- Streamlit App ---------------------------\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"Integrating whole pipeline\")\n",
        "# Option to upload or fetch from Firebase\n",
        "option = st.radio(\n",
        "    \"Choose an option:\",\n",
        "    (\"Upload an Image\", \"Fetch Random Images from Firebase\")\n",
        ")\n",
        "\n",
        "# Initialize a list to store the fetched images\n",
        "uploaded_files = []\n",
        "firebase_images = []\n",
        "\n",
        "if option == \"Fetch Random Images from Firebase\":\n",
        "    option1 = st.radio(\n",
        "        \"Choose the collection name for fetching images :\",\n",
        "        (\"balance_sheets\", \"bank_statements\", \"payslips\")\n",
        "    )\n",
        "\n",
        "    # Taking a number input from the user\n",
        "    num_images = st.number_input(\n",
        "        label=\"Enter the number of images to be fetched from Firebase\",\n",
        "        min_value=0,           # Minimum value\n",
        "        max_value=100,         # Maximum value\n",
        "        value=10,              # Default value\n",
        "        step=1,                # Step size for increments\n",
        "        format=\"%d\"            # Number format (\"%d\" for integers, \"%.2f\" for floats)\n",
        "    )\n",
        "    collection_name = option1\n",
        "    if not collection_name:\n",
        "        st.warning(\"Please enter a Firestore collection name.\")\n",
        "        st.stop()\n",
        "\n",
        "    if st.button(\"Fetch Images\"):\n",
        "        image_urls = get_image_urls_from_firestore(collection_name, num_images)\n",
        "        if not image_urls:\n",
        "            st.error(\"No images found in the specified collection.\")\n",
        "        else:\n",
        "            st.success(f\"Fetched {len(image_urls)} images.\")\n",
        "            for i, image_url in enumerate(image_urls):\n",
        "                st.write(f\"Image {i + 1}\")\n",
        "                st.image(image_url, caption=f\"Fetched Image {i + 1} from Firebase\")\n",
        "\n",
        "                # Fetch the image from the URL\n",
        "                response = requests.get(image_url)\n",
        "\n",
        "                if response.status_code != 200:\n",
        "                    st.error(f\"Failed to fetch image from {image_url}. HTTP Status: {response.status_code}\")\n",
        "                    continue\n",
        "\n",
        "                # Check Content-Type\n",
        "                if \"image\" not in response.headers.get(\"Content-Type\", \"\"):\n",
        "                    st.error(f\"URL {image_url} does not point to a valid image.\")\n",
        "                    continue\n",
        "\n",
        "                # Attempt to load the image\n",
        "                try:\n",
        "                    image = Image.open(BytesIO(response.content))  # Load image from URL\n",
        "                except UnidentifiedImageError:\n",
        "                    st.error(f\"Could not identify the image from {image_url}. Skipping...\")\n",
        "                    continue\n",
        "\n",
        "                # Convert the image data to a BytesIO object\n",
        "                img_byte_arr = BytesIO(response.content)\n",
        "                img_byte_arr.seek(0)\n",
        "\n",
        "                # Simulate an uploaded file\n",
        "                uploaded_file = {\n",
        "                    \"name\": f\"image_{i + 1}.png\",\n",
        "                    \"type\": \"image/png\",\n",
        "                    \"data\": img_byte_arr\n",
        "                }\n",
        "\n",
        "                # Append the simulated uploaded file to the list\n",
        "                firebase_images.append(uploaded_file)\n",
        "\n",
        "            # Now uploaded_files holds all the images as simulated uploaded file objects\n",
        "            st.write(f\"Total images stored as simulated UploadedFile objects: {len(firebase_images)}\")\n",
        "\n",
        "elif option == \"Upload an Image\":\n",
        "    uploaded_files = st.file_uploader(\"📂 Upload multiple images or PDFs\", type=[\"jpg\", \"jpeg\", \"png\", \"pdf\"], accept_multiple_files=True)\n",
        "    # Check file type based on the uploaded file's name or MIME type\n",
        "    if uploaded_files:\n",
        "        for uploaded_file in uploaded_files:\n",
        "            try:\n",
        "                # Determine file type\n",
        "                file_name = uploaded_file.name\n",
        "                if file_name.endswith(\".pdf\"):\n",
        "                    images = convert_from_bytes(uploaded_file.read())\n",
        "                    image = images[0]  # Use the first page for OCR\n",
        "                elif file_name.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "                    image = Image.open(uploaded_file)  # Load uploaded image\n",
        "                else:\n",
        "                    st.error(f\"Unsupported file type for {file_name}. Please upload PDFs or images.\")\n",
        "                    continue\n",
        "\n",
        "                # Display the image or the first page of the PDF\n",
        "                st.image(image, caption=f\"🖼️ Uploaded Image: {file_name}\", use_column_width=True)\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"❌ Error with {file_name}: {e}\")\n",
        "\n",
        "if firebase_images:\n",
        "    uploaded_files = firebase_images\n",
        "\n",
        "if uploaded_files:\n",
        "    st.sidebar.header(\"Global Preprocessing Defaults\")\n",
        "    contrast = st.sidebar.slider(\"Default Contrast Enhancement\", 1.0, 5.0, 2.0, 0.1)\n",
        "    sharpen = st.sidebar.slider(\"Default Sharpen Filter\", 0, 5, 1, 1)\n",
        "    median_filter = st.sidebar.slider(\"Default Median Filter Size\", 1, 5, 1, 1)\n",
        "\n",
        "    # Add sliders for hyperparameters\n",
        "    st.sidebar.subheader(\"PaddleOCR Hyperparameters\")\n",
        "    paddle_conf_threshold = st.sidebar.slider(\"Confidence Threshold\", 0, 100, 70)\n",
        "    use_angle_cls = st.sidebar.checkbox(\"Use Angle Classification\", True)\n",
        "    ocr_results = []\n",
        "\n",
        "    st.write(\"Performing OCR with PaddleOCR\")\n",
        "\n",
        "    for idx, uploaded_file in enumerate(uploaded_files):\n",
        "        try:\n",
        "            # Check if the file is from Firebase (dict) or uploaded (UploadedFile)\n",
        "            with st.expander(f\"Set Parameters for Image {idx + 1}\"):\n",
        "                if isinstance(uploaded_file, dict):  # Simulated Firebase file\n",
        "                    file_name = uploaded_file[\"name\"]\n",
        "                    img_byte_arr = uploaded_file[\"data\"]\n",
        "                    image = Image.open(img_byte_arr)  # Load the image from BytesIO\n",
        "                else:  # UploadedFile from Streamlit\n",
        "                    file_name = uploaded_file.name\n",
        "                    if uploaded_file.type == \"application/pdf\":\n",
        "                        images = convert_from_bytes(uploaded_file.read())\n",
        "                        image = images[0]  # Use the first page for OCR\n",
        "                    elif uploaded_file.type in [\"image/jpeg\", \"image/png\", \"image/jpg\"]:\n",
        "                        image = Image.open(uploaded_file)  # Load uploaded image\n",
        "                    else:\n",
        "                        st.error(f\"Unsupported file type: {uploaded_file.type}. Please upload PDFs or images.\")\n",
        "                        continue\n",
        "\n",
        "                # Display the uploaded image\n",
        "                st.image(image, caption=f\"Uploaded Image {idx + 1}\", use_column_width=True)\n",
        "\n",
        "                # Preprocess the image\n",
        "                preprocessed_image = preprocess_image(image, contrast, sharpen, median_filter)\n",
        "\n",
        "                np_image = np.array(preprocessed_image)\n",
        "                params = {\n",
        "                    \"min_size\": st.sidebar.slider(\"Min Size\", 1, 20, 10, 1, key=f\"min_size_{idx}\"),\n",
        "                    \"text_threshold\": st.sidebar.slider(\"Text Threshold\", 0.1, 1.0, 0.7, 0.05, key=f\"text_threshold_{idx}\"),\n",
        "                    \"low_text\": st.sidebar.slider(\"Low Text Threshold\", 0.1, 1.0, 0.4, 0.05, key=f\"low_text_{idx}\"),\n",
        "                    \"link_threshold\": st.sidebar.slider(\"Link Threshold\", 0.1, 1.0, 0.4, 0.1, key=f\"link_threshold_{idx}\"),\n",
        "                    \"canvas_size\": st.sidebar.slider(\"Canvas Size\", 2000, 5000, 2560, 10, key=f\"canvas_size_{idx}\"),\n",
        "                    \"mag_ratio\": st.sidebar.slider(\"Magnitude Ratio\", 0.1, 50.0, 1.0, 5.0, key=f\"mag_ratio_{idx}\"),\n",
        "                    \"slope_ths\": st.sidebar.slider(\"Slope Threshold\", 0.01, 1.0, 0.1, 0.01, key=f\"slope_ths_{idx}\"),\n",
        "                    \"ycenter_ths\": st.sidebar.slider(\"Y Center Threshold\", 0.1, 1.0, 0.5, 0.1, key=f\"ycenter_ths_{idx}\"),\n",
        "                    \"height_ths\": st.sidebar.slider(\"Height Threshold\", 0.1, 1.0, 0.5, 0.1, key=f\"height_ths_{idx}\"),\n",
        "                    \"width_ths\": st.sidebar.slider(\"Width Threshold\", 0.1, 1.0, 0.5, 0.1, key=f\"width_ths_{idx}\"),\n",
        "                    \"use_angle_cls\": use_angle_cls,\n",
        "                }\n",
        "\n",
        "                # Detect with PaddleOCR\n",
        "                paddle_image_with_boxes, paddle_boxes = detect_with_ppocr(preprocessed_image, params)\n",
        "                paddle_texts = [box[2] for box in paddle_boxes]\n",
        "                paddle_confidences = [box[3] for box in paddle_boxes]\n",
        "\n",
        "                # Display the OCR result image with bounding boxes\n",
        "                st.image(paddle_image_with_boxes, caption=f'PaddleOCR - Image {idx + 1} with Bounding Boxes', use_column_width=True)\n",
        "\n",
        "                # Display extracted texts and confidence scores\n",
        "                st.subheader(f\"PaddleOCR Results for Image {idx + 1}\")\n",
        "                paddle_df = pd.DataFrame({'Text': paddle_texts, 'Confidence': paddle_confidences})\n",
        "                st.dataframe(paddle_df)\n",
        "\n",
        "                # Add the OCR results to the list\n",
        "                ocr_results.append({'image_idx': idx + 1, 'texts': paddle_texts, 'confidence': paddle_confidences})\n",
        "\n",
        "                prompting_type = st.radio(\n",
        "                    f\"Type of Prompting for Image {idx + 1}\",\n",
        "                    options=[\"Manual\", \"Prompts keywords\"],\n",
        "                    index=0,\n",
        "                    key=f\"radio_{idx + 1}\"\n",
        "                )\n",
        "\n",
        "                if prompting_type == \"Manual\":\n",
        "                    custom_keywords = st.text_input(\n",
        "                        f\"Enter Custom Keywords for Image {idx + 1} (comma-separated)\",\n",
        "                        placeholder=\"e.g., Amount, Date, Balance\",\n",
        "                        key=f\"text_input_{idx + 1}\"\n",
        "                    )\n",
        "                else:\n",
        "                    custom_keywords = \"\"\n",
        "\n",
        "                document_type = st.radio(\n",
        "                    f\"Type of Financial Document for Image {idx + 1}\",\n",
        "                    options=[\"Payslips\", \"Bank Statement\", \"Balance Sheet\", \"Other\"],\n",
        "                    index=0,\n",
        "                    key=f\"doc_type_{idx + 1}\"\n",
        "                )\n",
        "\n",
        "                results, keyword_value_plots, excel_file = process_multiple_images(\n",
        "                    [uploaded_file], prompting_type, custom_keywords, document_type\n",
        "                )\n",
        "\n",
        "                st.text(\"Extracted Text:\")\n",
        "                st.write(results)\n",
        "                st.subheader(\"Keyword Value Plots:\")\n",
        "                if keyword_value_plots:\n",
        "                    for bar_plot, pie_plot, file_name in keyword_value_plots:  # Unpack the tuple\n",
        "                        if bar_plot:\n",
        "                            st.image(bar_plot, caption=f\"Bar Graph - {file_name}\", use_container_width=True)\n",
        "                        else:\n",
        "                            st.warning(f\"No bar plot available for {file_name}\")\n",
        "\n",
        "                        if pie_plot:\n",
        "                            st.image(pie_plot, caption=f\"Pie Chart - {file_name}\", use_container_width=True)\n",
        "                        else:\n",
        "                            st.warning(f\"No pie chart available for {file_name}\")\n",
        "                else:\n",
        "                    st.warning(\"No plots were generated.\")\n",
        "\n",
        "                # Provide download link for Excel file\n",
        "                st.text(\"Download Extracted Results:\")\n",
        "                st.download_button(\n",
        "                    label=\"Download Excel\",\n",
        "                    data=open(excel_file, 'rb').read(),\n",
        "                    file_name=\"extracted_results.xlsx\",\n",
        "                    mime=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\",\n",
        "                    key=f\"button_{idx + 1}\"\n",
        "                )\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Error processing file {file_name}: {e}\")\n",
        "\n",
        "    # Optionally: Provide a download option for the OCR results\n",
        "    if ocr_results:\n",
        "        # Convert the OCR results into text\n",
        "        ocr_text = \"\\n\\n\".join([f\"Image {result['image_idx']}:\\n\" + \"\\n\".join(result['texts']) for result in ocr_results])\n",
        "        st.download_button(\n",
        "            label=\"Download OCR Texts\",\n",
        "            data=ocr_text,\n",
        "            file_name=\"paddleocr_extracted_texts.txt\",\n",
        "            mime=\"text/plain\"\n",
        "        )\n",
        "    # Perform EDA on the extracted results\n",
        "    st.header(\"Perform Exploratory Data Analysis (EDA)\")\n",
        "    if 'excel_file' in locals() and excel_file:\n",
        "        df = pd.read_excel(excel_file)\n",
        "        exploratory_data_analysis(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill ngrok\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "!ngrok authtoken 2pL239b5flKDjRaU5JJEIh4R9vl_6FVYELEE3XsoePDxjJBEF\n",
        "!streamlit run app.py &>/dev/null&\n",
        "url = ngrok.connect(8501, \"http\")\n",
        "\n",
        "print(f\"Access your app here: {url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udksQFHm9FnG",
        "outputId": "3db2a859-f195-4f2f-c423-99780a9aecbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "Access your app here: NgrokTunnel: \"https://ea34-34-82-156-202.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/pranavrockz/OCR-Of-Bank-Statements-.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tT0zEp3wS4t",
        "outputId": "d64ac271-55ee-4ddf-cd58-4497db41cfc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'OCR-Of-Bank-Statements-'...\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 11 (delta 1), reused 3 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (11/11), 32.18 KiB | 784.00 KiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://colab.research.google.com/drive/1cZl2t7JSe59FJ1jH6T3VPeJOJu0o8PR5/export?format=ipynb\" -O Pipeline.ipynb\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA-oI5VBxjfi",
        "outputId": "4dfcccd1-d78a-4808-c9a4-94cbaaa2879f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-06 04:24:34--  https://colab.research.google.com/drive/1cZl2t7JSe59FJ1jH6T3VPeJOJu0o8PR5/export?format=ipynb\n",
            "Resolving colab.research.google.com (colab.research.google.com)... 216.239.36.180, 216.239.34.180, 216.239.38.180, ...\n",
            "Connecting to colab.research.google.com (colab.research.google.com)|216.239.36.180|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘Pipeline.ipynb’\n",
            "\n",
            "\rPipeline.ipynb          [<=>                 ]       0  --.-KB/s               \rPipeline.ipynb          [ <=>                ]  88.65K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2025-01-06 04:24:35 (18.0 MB/s) - ‘Pipeline.ipynb’ saved [90782]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/Pipeline.ipynb /content/OCR-Of-Bank-Statements-/"
      ],
      "metadata": {
        "id": "npA_Ad73w2Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/OCR-Of-Bank-Statements-/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vSGGRvAykkH",
        "outputId": "e067cbc2-9ae9-4159-f28a-f3b743abd4be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/OCR-Of-Bank-Statements-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!git remote set-url origin https://pranavrockz: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@github.com/pranavrockz/OCR-Of-Bank-Statements-.git\n"
      ],
      "metadata": {
        "id": "75lILcRW0FsE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote -v\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX4edTw40mN_",
        "outputId": "728e4f0e-7ebb-4ac8-e797-d4e9124476f1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "origin\thttps://pranavrockz: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@github.com/pranavrockz/OCR-Of-Bank-Statements-.git (fetch)\n",
            "origin\thttps://pranavrockz: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@github.com/pranavrockz/OCR-Of-Bank-Statements-.git (push)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!git pull origin main --rebase\n",
        "!git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oU7tmI3hysGk",
        "outputId": "7a5494ce-075b-45ef-a5c2-93ebba8d2534"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From https://github.com/pranavrockz/OCR-Of-Bank-Statements-\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Current branch main is up to date.\n",
            "Enumerating objects: 4, done.\n",
            "Counting objects: 100% (4/4), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (3/3), done.\n",
            "Writing objects: 100% (3/3), 28.96 KiB | 4.83 MiB/s, done.\n",
            "Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\n",
            "To https://github.com/pranavrockz/OCR-Of-Bank-Statements-.git\n",
            "   f78e7f1..f421b59  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nbformat\n",
        "!python -m nbformat validate /content/Pipeline.ipynb\n"
      ],
      "metadata": {
        "id": "6b0krPDY-v89",
        "outputId": "e1dd303f-ec89-49d7-ef25-7e04629416e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (5.10.4)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat) (4.23.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbformat) (5.7.2)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbformat) (5.7.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (0.22.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.3.6)\n",
            "/usr/bin/python3: No module named nbformat.__main__; 'nbformat' is a package and cannot be directly executed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to notebook --inplace /content/OCR-Of-Bank-Statements-/Pipeline.ipynb\n"
      ],
      "metadata": {
        "id": "5-vh8x74-6_R",
        "outputId": "6fc0a58f-0e1e-43e1-f347-fdda393f4bb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook /content/OCR-Of-Bank-Statements-/Pipeline.ipynb to notebook\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nbformat/reader.py\", line 19, in parse_json\n",
            "    nb_dict = json.loads(s, **kwargs)\n",
            "  File \"/usr/lib/python3.10/json/__init__.py\", line 346, in loads\n",
            "    return _default_decoder.decode(s)\n",
            "  File \"/usr/lib/python3.10/json/decoder.py\", line 337, in decode\n",
            "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
            "  File \"/usr/lib/python3.10/json/decoder.py\", line 355, in raw_decode\n",
            "    raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n",
            "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/jupyter-nbconvert\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jupyter_core/application.py\", line 283, in launch_instance\n",
            "    super().launch_instance(argv=argv, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nbconvert/nbconvertapp.py\", line 420, in start\n",
            "    self.convert_notebooks()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nbconvert/nbconvertapp.py\", line 597, in convert_notebooks\n",
            "    self.convert_single_notebook(notebook_filename)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nbconvert/nbconvertapp.py\", line 563, in convert_single_notebook\n",
            "    output, resources = self.export_single_notebook(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nbconvert/nbconvertapp.py\", line 487, in export_single_notebook\n",
            "    output, resources = self.exporter.from_filename(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nbconvert/exporters/exporter.py\", line 201, in from_filename\n",
            "    return self.from_file(f, resources=resources, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nbconvert/exporters/exporter.py\", line 221, in from_file\n",
            "    nbformat.read(file_stream, as_version=4), resources=resources, **kw\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nbformat/__init__.py\", line 174, in read\n",
            "    return reads(buf, as_version, capture_validation_error, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nbformat/__init__.py\", line 92, in reads\n",
            "    nb = reader.reads(s, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nbformat/reader.py\", line 75, in reads\n",
            "    nb_dict = parse_json(s, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/nbformat/reader.py\", line 25, in parse_json\n",
            "    raise NotJSONError(message) from e\n",
            "nbformat.reader.NotJSONError: Notebook does not appear to be JSON: '<!DOCTYPE html><html lang=\"en\"><head><s...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"pbhatnagar07@gmail.com\"\n",
        "!git config --global user.name \"pranavrockz\"\n"
      ],
      "metadata": {
        "id": "YLsGi00gy7Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global --list\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79dk2YtdzOKR",
        "outputId": "0631cc33-cb96-4a1b-efe4-144a58951776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user.email=pbhatnagar07@gmail.com\n",
            "user.name=pranavrockz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR9bwUP5AlYB"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GGXOe0PVa2r"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}